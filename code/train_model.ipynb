{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_Xy(X, y):\n",
    "    shuffled_index = np.arange(X.shape[0])\n",
    "    np.random.shuffle(shuffled_index)\n",
    "    return X[shuffled_index], y[shuffled_index]\n",
    "\n",
    "def record_loss(X, y, model, criterion, optimizer):\n",
    "    \"\"\" Compute loss with no gradients for the train/test score record \"\"\"\n",
    "    with torch.no_grad():\n",
    "        weighted_sum_loss = 0\n",
    "        batch_size = 512\n",
    "        batch_start = 0\n",
    "        while batch_start < X.shape[0]:\n",
    "            batch_end = batch_start + batch_size\n",
    "            Xb = X[batch_start:batch_end]\n",
    "            yb = y[batch_start:batch_end]\n",
    "            weight = Xb.shape[0] / X.shape[0]\n",
    "            weighted_sum_loss += weight * criterion(model.forward(Xb), yb)\n",
    "            batch_start += batch_size\n",
    "        if optimizer is not None:\n",
    "            optimizer.zero_grad()\n",
    "        return weighted_sum_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load and shuffle data\n",
      "(63620027, 4) (63620027,)\n",
      "Make mini train/val datasets\n",
      "Train:  (20000000, 4) (20000000,)\n",
      "Val:  (50000, 4) (50000,)\n"
     ]
    }
   ],
   "source": [
    "# Create train/val sets\n",
    "TRAIN_SIZE = 20 * int(1e6)\n",
    "VAL_SIZE = 50000\n",
    "\n",
    "print('Load and shuffle data')\n",
    "X_full, y_full = np.load('X_subsampled.npy'), np.load('y_subsampled.npy')\n",
    "print(X_full.shape, y_full.shape)\n",
    "\n",
    "print('Make mini train/val datasets')\n",
    "np.random.seed(314159)\n",
    "X_full, y_full = shuffle_Xy(X_full, y_full)\n",
    "X_train, y_train = X_full[:TRAIN_SIZE], y_full[:TRAIN_SIZE]\n",
    "print('Train: ', X_train.shape, y_train.shape)\n",
    "X_val, y_val = X_full[TRAIN_SIZE : TRAIN_SIZE + VAL_SIZE], y_full[TRAIN_SIZE : TRAIN_SIZE + VAL_SIZE]\n",
    "print('Val: ', X_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init model\n",
    "\n",
    "model = CBOW_Model(V=len(vocabulary), E=100, CW=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000000 training samples\n"
     ]
    }
   ],
   "source": [
    "# Init training\n",
    "\n",
    "trainRecord = []\n",
    "valRecord = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can reinitialize optimizer, changing params (e.g. decrease learning rate) as desired.\n",
    "# However, note that Adam learns certain update rules during training which will be lost\n",
    "# if this cell is rerun.\n",
    "\n",
    "# optimizer = optim.SGD(model.parameters(), lr=1, momentum=1e-3, nesterov=True)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000000 training samples\n",
      "\n",
      "---\n",
      "Epoch 112\n",
      "\n",
      "Batch 11776000  :11776512  ----  Loss: 5.215839862823486\n",
      "Batch 11904000  :11904512  ----  Loss: 5.262389183044434\n",
      "Batch 12032000  :12032512  ----  Loss: 5.42385196685791\n",
      "Batch 12160000  :12160512  ----  Loss: 5.481002330780029\n",
      "\n",
      "Epoch train loss: 5.396074295043945\n",
      "Epoch val loss:   5.434919834136963\n",
      "\n",
      "Batch 12288000  :12288512  ----  Loss: 5.431310653686523\n",
      "Batch 12416000  :12416512  ----  Loss: 5.365808486938477\n",
      "Batch 12544000  :12544512  ----  Loss: 5.330430507659912\n",
      "Batch 12672000  :12672512  ----  Loss: 5.349103927612305\n",
      "\n",
      "Epoch train loss: 5.3934006690979\n",
      "Epoch val loss:   5.431775093078613\n",
      "\n",
      "Batch 12800000  :12800512  ----  Loss: 5.475517272949219\n",
      "Batch 12928000  :12928512  ----  Loss: 5.440932750701904\n",
      "Batch 13056000  :13056512  ----  Loss: 5.462315082550049\n",
      "Batch 13184000  :13184512  ----  Loss: 5.4678053855896\n",
      "\n",
      "Epoch train loss: 5.390590190887451\n",
      "Epoch val loss:   5.428798675537109\n",
      "\n",
      "Batch 13312000  :13312512  ----  Loss: 5.720592021942139\n",
      "Batch 13440000  :13440512  ----  Loss: 5.4525909423828125\n",
      "Batch 13568000  :13568512  ----  Loss: 5.217245578765869\n",
      "Batch 13696000  :13696512  ----  Loss: 5.506192684173584\n",
      "\n",
      "Epoch train loss: 5.387925624847412\n",
      "Epoch val loss:   5.425856590270996\n",
      "\n",
      "Batch 13824000  :13824512  ----  Loss: 5.398845672607422\n",
      "Batch 13952000  :13952512  ----  Loss: 5.31869649887085\n",
      "Batch 14080000  :14080512  ----  Loss: 5.197205066680908\n",
      "Batch 14208000  :14208512  ----  Loss: 5.33957052230835\n",
      "\n",
      "Epoch train loss: 5.38522481918335\n",
      "Epoch val loss:   5.423130989074707\n",
      "\n",
      "Batch 14336000  :14336512  ----  Loss: 5.259726047515869\n",
      "Batch 14464000  :14464512  ----  Loss: 5.3232951164245605\n",
      "Batch 14592000  :14592512  ----  Loss: 5.49090051651001\n",
      "Batch 14720000  :14720512  ----  Loss: 5.5490922927856445\n",
      "\n",
      "Epoch train loss: 5.382363796234131\n",
      "Epoch val loss:   5.420013427734375\n",
      "\n",
      "Batch 14848000  :14848512  ----  Loss: 5.450273036956787\n",
      "Batch 14976000  :14976512  ----  Loss: 5.242023944854736\n",
      "Batch 15104000  :15104512  ----  Loss: 5.174413204193115\n",
      "Batch 15232000  :15232512  ----  Loss: 5.436746597290039\n",
      "\n",
      "Epoch train loss: 5.3796706199646\n",
      "Epoch val loss:   5.417264461517334\n",
      "\n",
      "Batch 15360000  :15360512  ----  Loss: 5.449023723602295\n",
      "Batch 15488000  :15488512  ----  Loss: 5.1378703117370605\n",
      "Batch 15616000  :15616512  ----  Loss: 5.171168327331543\n",
      "Batch 15744000  :15744512  ----  Loss: 5.275722980499268\n",
      "\n",
      "Epoch train loss: 5.37721061706543\n",
      "Epoch val loss:   5.414336204528809\n",
      "\n",
      "Batch 15872000  :15872512  ----  Loss: 5.4691314697265625\n",
      "Batch 16000000  :16000512  ----  Loss: 5.240138053894043\n",
      "Batch 16128000  :16128512  ----  Loss: 5.215341567993164\n",
      "Batch 16256000  :16256512  ----  Loss: 5.306972503662109\n",
      "\n",
      "Epoch train loss: 5.374695777893066\n",
      "Epoch val loss:   5.411477565765381\n",
      "\n",
      "Batch 16384000  :16384512  ----  Loss: 5.535165786743164\n",
      "Batch 16512000  :16512512  ----  Loss: 5.420259475708008\n",
      "Batch 16640000  :16640512  ----  Loss: 5.275280952453613\n",
      "Batch 16768000  :16768512  ----  Loss: 5.404713153839111\n",
      "\n",
      "Epoch train loss: 5.372351169586182\n",
      "Epoch val loss:   5.4085493087768555\n",
      "\n",
      "Batch 16896000  :16896512  ----  Loss: 5.367990970611572\n",
      "Batch 17024000  :17024512  ----  Loss: 5.330384254455566\n",
      "Batch 17152000  :17152512  ----  Loss: 5.47628116607666\n",
      "Batch 17280000  :17280512  ----  Loss: 5.360581398010254\n",
      "\n",
      "Epoch train loss: 5.369762420654297\n",
      "Epoch val loss:   5.4056901931762695\n",
      "\n",
      "Batch 17408000  :17408512  ----  Loss: 5.32951021194458\n",
      "Batch 17536000  :17536512  ----  Loss: 5.276767730712891\n",
      "Batch 17664000  :17664512  ----  Loss: 5.398359298706055\n",
      "Batch 17792000  :17792512  ----  Loss: 5.385988712310791\n",
      "\n",
      "Epoch train loss: 5.367559432983398\n",
      "Epoch val loss:   5.40310001373291\n",
      "\n",
      "Batch 17920000  :17920512  ----  Loss: 5.166191577911377\n",
      "Batch 18048000  :18048512  ----  Loss: 5.381382465362549\n",
      "Batch 18176000  :18176512  ----  Loss: 5.316534996032715\n",
      "Batch 18304000  :18304512  ----  Loss: 5.320798397064209\n",
      "\n",
      "Epoch train loss: 5.365061283111572\n",
      "Epoch val loss:   5.400170803070068\n",
      "\n",
      "Batch 18432000  :18432512  ----  Loss: 5.435240745544434\n",
      "Batch 18560000  :18560512  ----  Loss: 5.376767158508301\n",
      "Batch 18688000  :18688512  ----  Loss: 5.145132064819336\n",
      "Batch 18816000  :18816512  ----  Loss: 5.342335224151611\n",
      "\n",
      "Epoch train loss: 5.362684726715088\n",
      "Epoch val loss:   5.397457599639893\n",
      "\n",
      "Batch 18944000  :18944512  ----  Loss: 5.388766765594482\n",
      "Batch 19072000  :19072512  ----  Loss: 5.33235502243042\n",
      "Batch 19200000  :19200512  ----  Loss: 5.591761589050293\n",
      "Batch 19328000  :19328512  ----  Loss: 5.4402079582214355\n",
      "\n",
      "Epoch train loss: 5.3599653244018555\n",
      "Epoch val loss:   5.394936561584473\n",
      "\n",
      "Batch 19456000  :19456512  ----  Loss: 5.339227676391602\n",
      "Batch 19584000  :19584512  ----  Loss: 5.395463466644287\n",
      "Batch 19712000  :19712512  ----  Loss: 5.378072738647461\n",
      "Batch 19840000  :19840512  ----  Loss: 5.490574836730957\n",
      "\n",
      "Epoch train loss: 5.357415199279785\n",
      "Epoch val loss:   5.392139911651611\n",
      "\n",
      "Batch 19968000  :19968512  ----  Loss: 5.219911575317383\n",
      "\n",
      "Epoch train loss: 5.357173442840576\n",
      "Epoch val loss:   5.392024993896484\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "\n",
    "X, y = X_train, y_train\n",
    "N = X.shape[0]\n",
    "print(f'{N} training samples')\n",
    "criterion = F.cross_entropy\n",
    "\n",
    "BATCH_SIZE = 512\n",
    "NUM_EPOCHS = 1\n",
    "START_EPOCH = len(trainRecord)\n",
    "PRINT_INTERVAL = 250         # Print batch loss every <param> batches\n",
    "COMPUTE_VAL_INTERVAL = 1000  # Compute validation loss every <param> batches\n",
    "\n",
    "def print_append_epoch_loss():\n",
    "    \"\"\" Print and store train/val loss without gradient computation \"\"\"\n",
    "    \n",
    "    trainLoss = record_loss(\n",
    "        torch.LongTensor(X[:50000]), torch.LongTensor(y[:50000]), model, criterion, optimizer\n",
    "    )\n",
    "    valLoss = record_loss(\n",
    "        torch.LongTensor(X_val), torch.LongTensor(y_val), model, criterion, optimizer\n",
    "    )\n",
    "    trainRecord.append(trainLoss)\n",
    "    valRecord.append(valLoss)\n",
    "    print(f'Epoch train loss: { trainLoss }')\n",
    "    print(f'Epoch val loss:   { valLoss }')\n",
    "\n",
    "###\n",
    "### Training loop\n",
    "###\n",
    "for epoch in range(START_EPOCH, START_EPOCH + NUM_EPOCHS):\n",
    "    print(f'\\n---\\nEpoch {epoch}\\n')\n",
    "    \n",
    "    # Shuffle train data to randomize batches\n",
    "    X, y = shuffle_Xy(X, y)\n",
    "    \n",
    "    # Store initial train/val loss\n",
    "    if epoch == 0:\n",
    "        print_append_epoch_loss()\n",
    "        print('')\n",
    "    \n",
    "    print_counter = 0\n",
    "    val_counter = 0\n",
    "    batch_start = 11776000 ### Change this to resume mid-epoch at a certain batch.\n",
    "                    ### But if this is NOT zero, you should set NUM_EPOCHS to 1.\n",
    "    while batch_start < N:\n",
    "        # Get batch\n",
    "        batch_end = batch_start + BATCH_SIZE\n",
    "        Xb = torch.LongTensor(X[batch_start:batch_end])\n",
    "        yb = torch.LongTensor(y[batch_start:batch_end])\n",
    "        \n",
    "        # Compute loss, backprop\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(model.forward(Xb), yb)\n",
    "        \n",
    "        # Print batch progress, loss\n",
    "        if print_counter % PRINT_INTERVAL == 0:\n",
    "            print(f'Batch {batch_start:6}  :{min(batch_end, N):6}  ----  Loss: {loss}')\n",
    "        print_counter += 1\n",
    "        \n",
    "        # Print and store train/val loss\n",
    "        val_counter += 1\n",
    "        if val_counter % COMPUTE_VAL_INTERVAL == 0:\n",
    "            print('')\n",
    "            print_append_epoch_loss()\n",
    "            print('')\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Increment batch_start/loop counter\n",
    "        batch_start += BATCH_SIZE\n",
    "    \n",
    "    # Store epoch train/val loss\n",
    "    print('')\n",
    "    print_append_epoch_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final validation loss: 5.392024993896484\n",
      "Min validation loss:   5.392024993896484\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAD4CAYAAAD1jb0+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAgAElEQVR4nO3deXxU1f3/8dfJZJLJOllIwhJk0T5QCAFiRCwIIta61LW2FaVa22pt/VmX9veTqq1LtT9r/VlK67fWqtRWKrWiQq1Iq6XiigKyIyKyGJaQhOz7JOf3x52EhCxAtsnNvJ+Px33cmTs3cz+5hHdOztx7jrHWIiIi7hMR6gJERKRrFOAiIi6lABcRcSkFuIiISynARURcKrIvDzZo0CA7cuTIvjykiIjrrVmzptBam3bk9j4N8JEjR7J69eq+PKSIiOsZY3a3t11dKCIiLqUAFxFxKQW4iIhLHbUP3BjzNPAV4KC1Niu4LQX4GzAS2AV83Vpb3HtlikhX1NfXk5eXR01NTahLkWPg8/nIzMzE6/Ue0/7H8iHmn4DfAX9usW0u8Ia19iFjzNzg8zuOs1YR6WV5eXkkJCQwcuRIjDGhLkc6Ya2lqKiIvLw8Ro0adUxfc9QuFGvtSuDQEZsvAZ4JPn4GuPR4ChWRvlFTU0NqaqrC2wWMMaSmph7XX0td7QPPsNbuBwiu0zsp6gZjzGpjzOqCgoIuHk5Eukrh7R7H+2/V6x9iWmufsNbmWmtz09LaXId+TP777BLeeHJhD1cmIuJuXQ3wfGPMEIDg+mDPldRW7P6nGFb+SG8eQkR6QVFRERMnTmTixIkMHjyYYcOGNT+vq6s7pve47rrr2LZtW6f7PPbYYyxc2DONvGnTprFu3boeea/e1tU7MZcC1wIPBddLeqyidtSadJJ8uoNTxG1SU1Obw/Dee+8lPj6eH//4x632sdZirSUiov325IIFC456nJtuuqn7xbrQUVvgxpjngPeAMcaYPGPMd3CC+0vGmO3Al4LPe02DN4PUuAIaGxp78zAi0kc+/fRTsrKyuPHGG8nJyWH//v3ccMMN5ObmMm7cOO6///7mfZtaxIFAgKSkJObOncuECRM444wzOHjQ+eP/7rvvZt68ec37z507l8mTJzNmzBjeffddACorK/nqV7/KhAkTmD17Nrm5uUdtaT/77LOMHz+erKws7rzzTgACgQDf/OY3m7fPnz8fgF//+teMHTuWCRMmMGfOnB4/Z+05agvcWju7g5dm9XAtHTIx6XgjAxw6WEzKkNS+OqzIgHLrrdDTPQMTJ0IwN4/bli1bWLBgAY8//jgADz30ECkpKQQCAWbOnMkVV1zB2LFjW31NaWkpM2bM4KGHHuL222/n6aefZu7cuW3e21rLBx98wNKlS7n//vt57bXX+O1vf8vgwYNZvHgx69evJycnp9P68vLyuPvuu1m9ejV+v59zzjmHV155hbS0NAoLC9m4cSMAJSUlADz88MPs3r2bqKio5m29zRV3YkbGZwBQfKBXu9pFpA+deOKJnHbaac3Pn3vuOXJycsjJyWHr1q1s2bKlzdfExMRw/vnnA3Dqqaeya9eudt/78ssvb7PP22+/zZVXXgnAhAkTGDduXKf1rVq1irPPPptBgwbh9Xq56qqrWLlyJSeddBLbtm3jlltuYfny5fj9fgDGjRvHnDlzWLhw4THfiNNdfToaYVfFJKdDMVQU5AOnhLocEVfqaku5t8TFxTU/3r59O7/5zW/44IMPSEpKYs6cOe1eDx0VFdX82OPxEAgE2n3v6OjoNvsc7wTuHe2fmprKhg0bWLZsGfPnz2fx4sU88cQTLF++nDfffJMlS5bwwAMPsGnTJjwez3Ed83i5ogUeP8hpgVcX54e4EhHpDWVlZSQkJJCYmMj+/ftZvnx5jx9j2rRpPP/88wBs3Lix3RZ+S1OmTGHFihUUFRURCARYtGgRM2bMoKCgAGstX/va17jvvvtYu3YtDQ0N5OXlcfbZZ/OrX/2KgoICqqqqevx7OJIrWuBJg9NhO9SXqwtFZCDKyclh7NixZGVlMXr0aKZOndrjx7j55pu55ppryM7OJicnh6ysrObuj/ZkZmZy//33c9ZZZ2Gt5aKLLuLCCy9k7dq1fOc738FaizGGX/7ylwQCAa666irKy8tpbGzkjjvuICEhoce/hyOZ4/2zojtyc3NtVyZ0CNQ3Yv7m5Z3inzD95gd6oTKRgWnr1q2ccoq6HcG5eiQQCODz+di+fTvnnnsu27dvJzKyf7Vj2/s3M8assdbmHrlv/6q8A5HeCPIr0oioVwtcRLqmoqKCWbNmEQgEsNbyhz/8od+F9/FyTfXF1RlEN6oPXES6JikpiTVr1oS6jB7lig8xASrq04mJUAtcRKSJawK82mbgj1YLXESkiWsCvN6TTnKsWuAiIk1cE+CNURnER1dSV1UZ6lJERPoF1wS4Jy54O/1+tcJF3OKss85qc1POvHnz+MEPftDp18XHxwOwb98+rrjiig7f+2iXJc+bN6/VDTUXXHBBj4xTcu+99/LII6Ef4to1AR6V6Ez6U5qvfnARt5g9ezaLFi1qtW3RokXMnt3RGHmtDR06lBdeeKHLxz8ywF999VWSkpK6/H79jWsCPDbVaYFXHVILXMQtrrjiCl555RVqa2sB2LVrF/v27WPatGnN12Xn5OQwfvx4lixpO63Arl27yMrKAqC6uporr7yS7OxsvvGNb1BdXd283/e///3moWjvueceAObPn8++ffuYOXMmM2fOBGDkyJEUFhYC8Oijj5KVlUVWVlbzULS7du3ilFNO4frrr2fcuHGce+65rY7TnnXr1jFlyhSys7O57LLLKC4ubj7+2LFjyc7Obh5E680332ye0GLSpEmUl5d3+dyCi64D96enwwGoKVULXKRL1twKxT08nmzyRDi141GyUlNTmTx5Mq+99hqXXHIJixYt4hvf+AbGGHw+Hy+99BKJiYkUFhYyZcoULr744g7nhfz9739PbGwsGzZsYMOGDa2Gg33wwQdJSUmhoaGBWbNmsWHDBn74wx/y6KOPsmLFCgYNGtTqvdasWcOCBQtYtWoV1lpOP/10ZsyYQXJyMtu3b+e5557jj3/8I1//+tdZvHhxp+N7X3PNNfz2t79lxowZ/OxnP+O+++5j3rx5PPTQQ+zcuZPo6OjmbptHHnmExx57jKlTp1JRUYHP5zues92Ga1rgKUOcLpSGSrXARdykZTdKy+4Tay133nkn2dnZnHPOOezdu5f8TrpIV65c2Ryk2dnZZGdnN7/2/PPPk5OTw6RJk9i8efNRB6p6++23ueyyy4iLiyM+Pp7LL7+ct956C4BRo0YxceJEoPMha8EZn7ykpIQZM2YAcO2117Jy5crmGq+++mqeffbZ5js+p06dyu233878+fMpKSnp9p2g7mmBp0RTUuXH1KgFLtIlnbSUe9Oll17K7bffztq1a6murm5uOS9cuJCCggLWrFmD1+tl5MiR7Q4h21J7rfOdO3fyyCOP8OGHH5KcnMy3vvWto75PZ2NANQ1FC85wtEfrQunIP//5T1auXMnSpUv5+c9/zubNm5k7dy4XXnghr776KlOmTOH111/n5JNP7tL7g4ta4MZAUWUGkQ1qgYu4SXx8PGeddRbf/va3W314WVpaSnp6Ol6vlxUrVrB79+5O32f69OnNExdv2rSJDRs2AM5QtHFxcfj9fvLz81m2bFnz1yQkJLTbzzx9+nRefvllqqqqqKys5KWXXuLMM8887u/N7/eTnJzc3Hr/y1/+wowZM2hsbOTzzz9n5syZPPzww5SUlFBRUcGOHTsYP348d9xxB7m5uXz88cfHfcyWXNMCByirTcfnVQtcxG1mz57N5Zdf3uqKlKuvvpqLLrqI3NxcJk6ceNSW6Pe//32uu+46srOzmThxIpMnTwac2XUmTZrEuHHj2gxFe8MNN3D++eczZMgQVqxY0bw9JyeHb33rW83v8d3vfpdJkyZ12l3SkWeeeYYbb7yRqqoqRo8ezYIFC2hoaGDOnDmUlpZireW2224jKSmJn/70p6xYsQKPx8PYsWObZxfqKlcMJ9vkrV9cwbCErYy+eXMPViUycGk4Wfc5nuFkXdOFAlBr0knyqQUuIgIuC/AGbwYpcUXQWB/qUkREQs5VAY7PuZSwqrgwxIWIuEdfdpNK9xzvv5WrAtybkAbAof0FIa5ExB18Ph9FRUUKcRew1lJUVHRcN/e46ioUX0IS1EBlcWmoSxFxhczMTPLy8igoUKPHDXw+H5mZmce8v8sC3A81UFOuABc5Fl6vl1GjRoW6DOklrupCiUn0A1BXqQAXEXFVgMcmOQEeqFaAi4i4KsDjk50Ab6xRgIuIuCrAE5N91NZHQX33Z9QQEXE7VwW41wtl1X5MQC1wERFXBThAea0fT6MCXETEdQFeVe/HaxXgIiKuC/DqBj/RRgEuItKtADfG3GKM2WSM2WyMubWniupMbaMfn0cBLiLS5QA3xmQB1wOTgQnAV4wxX+ipwjpSj59YrwJcRKQ7LfBTgPettVXW2gDwJnBZz5TVsYYIP/HRCnARke4E+CZgujEm1RgTC1wADD9yJ2PMDcaY1caY1T0xoE5jpJ8EXzk0NnT7vURE3KzLAW6t3Qr8Evg38BqwHgi0s98T1tpca21uWlpalwtt5nXuxqyvbjtRqYhIOOnWh5jW2qestTnW2unAIWB7z5TVsYjoJADKD6kbRUTCW3evQkkPrk8ALgee64miOuOJcVrglSUKcBEJb90dD3yxMSYVqAdustYW90BNnYqK80MtVJcpwEUkvHUrwK21Z/ZUIccqOt4J8FpN6iAiYc51d2LG+DWpg4gIuDDA45omdahSgItIeHNdgDdN6tCgSR1EJMy5LsD9Kc6kDrZeAS4i4c11Ae71Qmm1nwgFuIiEOdcFOEBFrZ8ITeogImHOlQGuSR1ERFwa4NUNfqIjNLGxiIQ3Vwa4JnUQEXFpgGtSBxERlwZ4IMJPfJQCXETCmysD3Ho0qYOIiCsDnKjg7fQ1mtRBRMKXKwPcBAO8oljdKCISvlwZ4J4YZ1YeBbiIhDNXBrg31mmBV5cqwEUkfLkywKMTnACv0aQOIhLGXBngMYma1EFExJUB3jypQ7UCXETClysDvGlSh0ZN6iAiYcyVAd48qUOdAlxEwpcrAzw6GkqqkogIaERCEQlfkaEuoKvKa5PwoAAXkfDlyhY4QGVdMl5bHOoyRERCxrUBXt2QhE+TOohIGHNtgNfYZGIj1QIXkfDl2gAPkEScVy1wEQlfrg3wBk8SCb4SsDbUpYiIhIRrA9x6k4mKrMcGqkJdiohISLg2wE20M6RsVYm6UUQkPLk2wCNjkwEoP6QPMkUkPLk2wL1xTgu8slgtcBEJT64NcF+CE+A1ZQpwEQlPrg3w2CSnC6WuQl0oIhKeuhXgxpjbjDGbjTGbjDHPGWN8PVXY0cQlOy3w+kq1wEUkPHU5wI0xw4AfArnW2izAA1zZU4UdTWKqE+CNNWqBi0h46m4XSiQQY4yJBGKBfd0v6dj4kyMpr47H1qkFLiLhqcsBbq3dCzwC7AH2A6XW2n8duZ8x5gZjzGpjzOqCgoKuV3qEyEgordaY4CISvrrThZIMXAKMAoYCccaYOUfuZ619wlqba63NTUtL63ql7SivTSZSQ8qKSJjqThfKOcBOa22BtbYeeBH4Ys+UdWyqAklEaVIHEQlT3QnwPcAUY0ysMcYAs4CtPVPWsaluSCYmQi1wEQlP3ekDXwW8AKwFNgbf64kequuY1JFErIaUFZEw1a05Ma219wD39FAtxy1gkomLUoCLSHhy7Z2YAI2RSfhjSqGxIdSliIj0OVcHOFHOzTwNtWUhLkREpO+5OsAjooNDyhbpg0wRCT+uDvCmIWUrNKSsiIQhdwd4vNMCr1KAi0gYcnWAxzSNCV6uLhQRCT+uDnANKSsi4czVAR6f4nShBKrUAheR8OPqAPenxhNo8NBYqxa4iIQfVwd4fIKhpCoJozHBRSQMdetW+lAzBspqkvCgLhQRCT+uboEDVNYlEWnVAheR8OP6AK8KJBNt1AIXkfDj+gCvbUwixqMWuIiEH9cHeJ1JJt6rFriIhB/XB3g1Q0iJOwgNdaEuRUSkT7k+wBt8o/BENFJXuifUpYiI9CnXB7hv0GgACj7bGeJKRET6lusDPDlzFAClez8LcSUiIn3L9QE+7AtDqa2PovaQWuAiEl5cH+BDh3nYUzSCiGoFuIiEF9cHeEQEHKgYRZxVF4qIhBfXBzhAacNoBvnUAheR8DIgArwuahRJMUVQr9npRSR8DIgA9yQ6lxJW5KsVLiLhY0AEeFyGcylhwU4FuIiEjwER4Gmjgi3w/fogU0TCx4AI8BNOSqak0k+gVC1wEQkfAyLAk5Nhz6FRRNWpBS4i4WNABDhAQfVoEiLUAheR8DFgAryCUaTH7gRrQ12KiEifGDAB3uAbjc9bg60+EOpSRET6xIAJ8KgU51LCot3qBxeR8DBgAjxuWBYA5bveD3ElIiJ9o8sBbowZY4xZ12IpM8bc2pPFHY+TsoezYc94Ig/8I1QliIj0qS4HuLV2m7V2orV2InAqUAW81GOVHafhw+H9zy9mqPdtqNMkxyIy8PVUF8osYIe1dncPvV+XNAy5CE9EAxXbl4WyDBGRPtFTAX4l8Fx7LxhjbjDGrDbGrC4oKOihw7Vv0qzTOFCSQdF6daOIyMDX7QA3xkQBFwN/b+91a+0T1tpca21uWlpadw/XqdMmR/DGxxcyqH4ZNNb36rFEREKtJ1rg5wNrrbX5PfBe3eLxQJHvIuK8pTTsfyvU5YiI9KqeCPDZdNB9EgojTv8SNXXR7F+9JNSliIj0qm4FuDEmFvgS8GLPlNN9Z58bxz/WXUJq2Z90NYqIDGjdCnBrbZW1NtVaW9pTBXVXQgK8V3onMZ4yajbMD3U5IiK9ZsDcidnSVTdN4KUPL8VunQd1/eZ3i4hIjxqQAZ6bC/8p+CkxnhJqN/021OWIiPSKARngAFfdlMPSNRfRuOVRqC0KdTkiIj1uwAb4GWfAK58/SKQtp27V7aEuR0Skxw3YAAe4/n+P5+F/3EFU3p9h3/JQlyMi0qMGdICfdhqUZN7N1r0nU/PW96C+ItQliYj0mAEd4AD3/tzHT195kqjAHgKrbtaUayIyYAz4AI+Lgxvumsr9L/6MyD1/gh1/DHVJIiI9YsAHOMC550JJ5s9Ytv48Gj64GQo/CHVJIiLdFhYBDvDwryKY98FCPi8cSv2Ky6ByT6hLEhHplrAJ8KgoeOovKXzzyX9QXV5J4PXzoPZQqMsSEemysAlwgMxM+MVjWXx13hIay3bQsOIiCFSGuiwRkS4JqwAHOPNMuPGeGVz9u4VQ+D6NK76iEBcRVwq7AAf46lfhrGuu4Ju//wscXKkQFxFXCssAB7jpJsi57Crm/E8wxN/4ksZMERFXCdsAB/jxj+G0K67iinl/J3BwLY3Lp0LFzlCXJSJyTMI6wAFuuw3Oue5yZj34OhWFB2l8bQoUvBvqskREjirsAxzgBz+AH/58Gmfe/w6fH0ik8fWZsOPpUJclItIpBXjQ174G8xacwqyHV/Hmlumw6juw6rv6cFNE+i0FeAszZ8Ibb6Xwk+XLePDlO2n89Gnsshw4tDbUpYmItKEAP8KIEfDflZHs8j/I2Q/+h8IDldjlU2Dr/wPbGOryRESaKcDb4fPBH/8Ic24/i+w71rN844Xw0Y9hxXm6SkVE+g0FeCe++1345+up3PbSi3zvqcep3fsu9p/jYPMvoKEu1OWJSJhTgB9FTg6sWWPwnvI9Trp1K6+tPx/W3wXLJkD+f0NdnoiEMQX4MYiNhd/9Dha+NJwfvbyYCx7+J/n7a+GNmfDO1VCxK9QlikgYUoAfh+nTYd06mPq1Czj5R5v41bK7COx6EfvKGFj7I92KLyJ9SgF+nKKi4K674MO1sfw7/wFG/fATFq+5GvvxPOzSE2HzQxCoCnWZIhIGFOBddNJJsHw5PP7n4dzz6tOMv2M973xyJqz/CSwdBVsehvryUJcpIgOYArwbjIELL4T16+GWn2Xxtd/9g2n3vcVHOyfCujtgyQjYeJ9m/hGRXqEA7wGRkXD99fDpp3DeN6cx68HlnPbTD3h3x3TYeK8T5Gtug4rPQl2qiAwgCvAeFBcHd98Nu3bBV68/jUseeZnxczfw+seX0Ljtd9ilJ8HKSyF/BVgb6nJFxOUU4L0gMRHmznWC/Lpbx3PN48+SedNu/vD2XVR//g68cbZzHfm230FdSajLFRGXUoD3org4uP122LkTHvrNUP7w3s9JuW4Pt/z1KfYdiIQ1N8NLQ+Dda+DgW2qVi8hxUYD3gehouOYaWLsWlv0rhl2eb5P53bXk3r2aV7deS/2ul+H16fDKGNhwD5RuDXXJIuICxnaj1WeMSQKeBLIAC3zbWvteR/vn5uba1atXd/l4A8muXfD0085SUljB9V9+nv914bOMjv8vBgtJE2DElTDiGxA/KtTlikgIGWPWWGtz22zvZoA/A7xlrX3SGBMFxFprO+zUVYC3FQjAsmXO6IevvgppCfu59bK/M2f6IoZFBX8Xpp7uBHnmpQpzkTDU4wFujEkE1gOj7TG+iQK8cwcPwvPPw7PPwqpVMDJtF3dc+TyX5y4iPfIjZ6ek8TDsEsi8BFJOdS5GF5EBrTcCfCLwBLAFmACsAW6x1lYesd8NwA0AJ5xwwqm7d+/u0vHCzfbt8Ne/wgsvwKZNMDp9BzdftoTLJy9huO9tDI0QMwwyL4bhl0PGLIW5yADVGwGeC7wPTLXWrjLG/AYos9b+tKOvUQu8az75BF58ERYvhtWrITW+kBsv+idXzVjCmMTleGwVJGXDuLtg+GUQ4Q11ySLSg3ojwAcD71trRwafnwnMtdZe2NHXKMC7b88eeOklJ8zfeQeiPNV87/y/8+BV/5e4ho8hMg7SpkH6DGdJyQVPVKjLFpFu6CjAu3wZobX2APC5MWZMcNMsnO4U6UUnnAC33AIrVzp95k8uiOFf26/Bf9Um/nZgCXbUt6AqD9bfCf+eCi8kw3++BJsegINvQ0NtqL8FEekh3b0KZSLOZYRRwGfAddba4o72Vwu8d1RUwI03wsKFkJoKM2bA+WcXcO7Etxge9V9MwZtQssHZ2eOD1Ckw6Izgcjr40kNav4h0rlcuIzxeCvDeYy28/DIsXQr//a9znTnAoEFw1lnw5ZlFnJP9Fif43iSi8C0oXg824OwUP/pwqKecCjEZEJUCXr8+GBXpBxTgYWbXLifIV6xwls8/d7YnJMDpp8P0L1bxpdy1ZA99n9iq96DwPaje3/pNotMgeRKk5Bxex48Goxt4RfqSAjyMWeuMx/LOO/Dee86yYQM0Njqvn3wynHGG5Zwv5jFlzHqS44qI8xRiyrdA8VoiKzZjbL2zc2QCpExyAj05x3mceApERIbuGxQZ4BTg0kpFBXz4oRPm774L778PRR1M6RkVWcuXT9/MeaevZfJJH3Fiylr8dj0RjdXODh4f+MdDcrazTgouvrS++4ZEBjAFuHTKWufmofXr4dAhKC4GjwdiY6G01JnMed06Zx+ACNPAlLHbuGjaR3zx5LWMSf+IVM9GIhsKD7+pL/1woMcMBW+iE+oJYyDhJPBEh+abFXEZBbj0iPJyp/vlo4+cZd06507RujoAy7DUfM6bspEzx28ie8RGRvg3ksRmImx16zcyERA3ChLHOIHuP9lZJ44BX4Y+PBVpQQEuvaauDj7+2AnyzZsPrz/7rGmIc0tcdCWJMWWcMXE/99y2jeyR26DsYyjbBuWfQEOLgPf6nSBPPBkSvgDxJzpLwonO1TEKdwkzCnDpc5WVsHUrbNkCBQVQVgaLFjlDA8yeDRdfDKNHw4mjG0nxfY4pbxHqZcHH1Xtbv6nXfzjMWwZ7/InO2DARntB8syK9SAEu/UJNDfziF/DLXzZ1uzj8/mCYn9h6fdLIKjKTPyOyegeU74CK4FK+Ayp3Hb6WHSAiyhluN/5E53LHuBEQe4Kzjhvh9MnrEkhxIQW49CtVVU4Xy44dbdc7d0J9/eF9IyNhxIi24X7i6AAnDskj3rYI9aaAr/gM6staHzQiKhjoJxwR7k3Ph+uDVemXFODiGg0NsHdv61Bv+bj4iMEa0tIOh/rIkTB8OGRmwoihpQxP2Y0/cg+majdU7obKPc66ak/wxqUjfv59g1sHeswwiB3WYj1UIS99TgEuA0ZxcdtWe9Oyd6/zC6ClmBgn0IcPPxzuw4fD8GF1jB6cx1D/bhIigiFfFQz4yt3OoGAN1W0LiB7UTrAH17GZzuOoZH3YKj2mowDX7XPiOsnJcOqpznKkQADy852hA/LynHXLx2+8Afv2Nd2FGgWMBkYTG9s63DMzYdgwywmDSxieupeMxL2k+PYSWbfX+WC1Krgu+hBqC9oW4vE5QR4z2Lksss0y2BlzxpfhDAEs0gUKcBlQIiNh2DBn6UggAAcOtA72lut//xv274fGRgMkB5cswBntccgQGDzYWQ8ZAplDahk5eD+ZKYeD3tcYDPiafOdqmoNvQm0Ht7pGxnUQ8unOEp0WXKdDdIo+iJVmCnAJO5GRTgs7M7PjfZpa8gcOOGHetLR8/sknzvO6umhgZHBxxMU5IZ+R4Szp6TAko54RGQcZNiifwf58BsXn44/OJ8YcwNTkO2Ffvh0K3g6GfTvdmybC6cKJTm8b8E2PowdBdKqzjkrRODUDmP5lRdpxLC15cG5UKi5uG+5Ny8GDzvADb78NhYVerB0GtH5Tr9cJ+KYlIwMGpwcYMaSIYakHyUgqIDXuIEm+g8R7D+KjAFN7EGoOQtFqpwunvrTjIiOiAONMtReV4rTio1KO/XFkTLfPp/QOBbhINxgDKSnOMm5c5/s2NEBhoRPq+fmt1y0fb90K+fmR1NZmABlt3sfjcY43aJDTpZOaCoPTajkho4BhgwrISCpiUEIhyXFF+KML8HpqAYvPW0e0KYbaQ1B3CMq2BB8XQWN9m+McPqCvk5BP7ST44/RBbi9TgIv0EY/ncJfK+PGd72utM+7MwYPOKJFFRXj32v0AAAeMSURBVE74t1w3Pf7sM/jgg2iKijKpq+u4X8jrheuugzvvdK6rb3Wwhiqn26bu0OGA7+hx+afB50XQ2MkUfRFe8CZBVNLhdUeP29vmidEvgKNQgIv0Q8ZAYqKznHTSsX2Ntc4wwS3DvbAQamud19asgaeecpahQ527X53F4PfHBZcTmrcnJrbcB/wZh7d7mkYsCFR3EvZFUFfiLPXBddWe4PNSaKjp/Btq9QvAf5Tw9zujXTYtkYngTRjw1+zrOnCRMJKXB48/7qxLSw8vZWWHH7cc4qAj8fHtBHw7y5H7tHwe5amButLD4d4y6JvXpR2/1t41+keKiGod7M3hHgz4VtsTnG3N6/jWz0P4y0DXgYsImZnwwAOd71NT0zrcjwz49pbiYmcav6bn1ceQrdHRPvx+H35/RptwT0x0pv9rubTZFldLoq+UOG8JBfvL2Lq+jPy9ZSREl5EQU0Z8dBlxkWX4TDk+W0Z0oIyohjIi7T48jR9jAmWYQNnR/xJoEuFtJ+Q7CPumdUR0sBvIwNDznF8UPUgBLiKt+HzOktH289NjVl/ffvC3fN7ea9u3H35cXn542r/2RQPpweX4GeP8JZGUWM/g1DLSkstJSyonNbGclMRykuIr8MeV448pJyGmnPjocmKjyonxlhPjKSfaU47XlOIlD48tx9NQDg3lGNvQ/gEv3Ap+BbiI9HNer3OVzKBBXX8Pa52WfHn54aUp2I9cUlPhjDMgO9v55VFRcTyLl4qKVCoqUtldClv2tX69vNy5L+AYqyYpoYaMlArSU8qJi6klwliMgfmTRzHa3/Xz0R4FuIj0S8Y4U/rFxh7fXwNRUc6NVN35C+JIdXXH+svAUFERQ2VlDBUVac0fIFsLvtieq6eJAlxE5Ciiog5f79+faFAFERGXUoCLiLiUAlxExKUU4CIiLqUAFxFxKQW4iIhLKcBFRFxKAS4i4lJ9OhqhMaYA2N3FLx8EFPZgOX3NzfW7uXZwd/1urh3cXX9/qn2EtTbtyI19GuDdYYxZ3d5wim7h5vrdXDu4u3431w7urt8NtasLRUTEpRTgIiIu5aYAfyLUBXSTm+t3c+3g7vrdXDu4u/5+X7tr+sBFRKQ1N7XARUSkBQW4iIhLuSLAjTHnGWO2GWM+NcbMDXU9nTHGDDfGrDDGbDXGbDbG3BLcnmKM+bcxZntwnRzqWjtijPEYYz4yxrwSfD7KGLMqWPvfjDFRoa6xI8aYJGPMC8aYj4P/Bme47NzfFvy52WSMec4Y4+vP598Y87Qx5qAxZlOLbe2eb+OYH/x/vMEYkxO6yjus/VfBn50NxpiXjDFJLV77SbD2bcaYL4em6tb6fYAbYzzAY8D5wFhgtjFmbGir6lQA+JG19hRgCnBTsN65wBvW2i8AbwSf91e3AFtbPP8l8Otg7cXAd0JS1bH5DfCatfZkYALO9+GKc2+MGQb8EMi11mYBHuBK+vf5/xNw3hHbOjrf5wNfCC43AL/voxo78ifa1v5vIMtamw18AvwEIPh/+EpgXPBr/ieYTSHV7wMcmAx8aq39zFpbBywCLglxTR2y1u631q4NPi7HCZBhODU/E9ztGeDS0FTYOWNMJnAh8GTwuQHOBl4I7tKfa08EpgNPAVhr66y1Jbjk3AdFAjHGmEggFthPPz7/1tqVwKEjNnd0vi8B/mwd7wNJxpghfVNpW+3Vbq39l7W2aQrj94HM4ONLgEXW2lpr7U7gU5xsCik3BPgw4PMWz/OC2/o9Y8xIYBKwCsiw1u4HJ+SB9NBV1ql5wP8BGoPPU4GSFj/U/fn8jwYKgAXBLqAnjTFxuOTcW2v3Ao8Ae3CCuxRYg3vOf5OOzrfb/i9/G1gWfNwva3dDgJt2tvX7ax+NMfHAYuBWa21ZqOs5FsaYrwAHrbVrWm5uZ9f+ev4jgRzg99baSUAl/bS7pD3BvuJLgFHAUCAOp9vhSP31/B+Na36WjDF34XSHLmza1M5uIa/dDQGeBwxv8TwT2BeiWo6JMcaLE94LrbUvBjfnN/25GFwfDFV9nZgKXGyM2YXTVXU2Tos8KfgnPfTv858H5FlrVwWfv4AT6G449wDnADuttQXW2nrgReCLuOf8N+nofLvi/7Ix5lrgK8DV9vCNMv2ydjcE+IfAF4KfxEfhfJCwNMQ1dSjYZ/wUsNVa+2iLl5YC1wYfXwss6evajsZa+xNrbaa1diTOef6PtfZqYAVwRXC3flk7gLX2APC5MWZMcNMsYAsuOPdBe4ApxpjY4M9RU/2uOP8tdHS+lwLXBK9GmQKUNnW19BfGmPOAO4CLrbVVLV5aClxpjIk2xozC+SD2g1DU2Iq1tt8vwAU4nwjvAO4KdT1HqXUazp9WG4B1weUCnL7kN4DtwXVKqGs9yvdxFvBK8PFonB/WT4G/A9Ghrq+TuicCq4Pn/2Ug2U3nHrgP+BjYBPwFiO7P5x94Dqe/vh6nlfqdjs43TjfEY8H/xxtxrrbpb7V/itPX3fR/9/EW+98VrH0bcH6oz721VrfSi4i4lRu6UEREpB0KcBERl1KAi4i4lAJcRMSlFOAiIi6lABcRcSkFuIiIS/1/z4AYhqrcTCwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot train/val loss\n",
    "\n",
    "print(f'Final validation loss: { valRecord[-1] }')\n",
    "print(f'Min validation loss:   { min(valRecord) }')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epochIndex = np.arange(len(trainRecord))\n",
    "plt.plot(epochIndex, trainRecord, color='blue', label='Training loss')\n",
    "plt.plot(epochIndex, valRecord, color='orange', label='Validation loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model, optimizer, train/val sets, train/val record\n",
    "\n",
    "### NOTE: start at epoch 0, batch 15744000\n",
    "### NOTE: finished epoch 0, start new epoch\n",
    "\n",
    "### NOTE: start at epoch 1, batch 11776000\n",
    "### NOTE: finished epoch 1\n",
    "\n",
    "SAVE_NAME = 'model_subsample_20M_2'\n",
    "\n",
    "allTrainData = {\n",
    "    'model': model,\n",
    "    'optimizer': optimizer,\n",
    "    'trainSets': (X_train, y_train),\n",
    "    'valSets': (X_val, y_val),\n",
    "    'trainRecord': trainRecord,\n",
    "    'valRecord': valRecord,\n",
    "}\n",
    "\n",
    "import pickle\n",
    "with open(SAVE_NAME + '.pkl', 'wb') as f:\n",
    "    pickle.dump(allTrainData, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model, optimizer, train/val sets, train/val record\n",
    "\n",
    "LOAD_NAME = 'model_subsample_20M_2'\n",
    "\n",
    "import pickle\n",
    "with open(LOAD_NAME + '.pkl', 'rb') as f:\n",
    "    savedData = pickle.load(f)\n",
    "    \n",
    "model = savedData['model']\n",
    "optimizer = savedData['optimizer']\n",
    "X_train, y_train = savedData['trainSets']\n",
    "X_val, y_val = savedData['valSets']\n",
    "trainRecord = savedData['trainRecord']\n",
    "valRecord = savedData['valRecord']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted        :   Actual           :   Equal?\n",
      "-------------------------------------------------\n",
      "forward          :   forward          :   Yes   \n",
      "construction     :   identification   :   No    \n",
      "estimate         :   estimate         :   Yes   \n",
      "reserve          :   serotonin        :   No    \n",
      "follow           :   periods          :   No    \n",
      "meet             :   hemophilia       :   No    \n",
      "sell             :   opportunities    :   No    \n",
      "primarily        :   increase         :   No    \n",
      "revenue          :   period           :   No    \n",
      "account          :   account          :   Yes   \n",
      "fasb             :   account          :   No    \n",
      "result           :   could            :   No    \n",
      "litigation       :   losses           :   No    \n",
      "cost             :   field            :   No    \n",
      "party            :   party            :   Yes   \n",
      "cash             :   cash             :   Yes   \n",
      "revenues         :   grow             :   No    \n",
      "report           :   begin            :   No    \n",
      "approval         :   depend           :   No    \n",
      "merrill          :   merrill          :   Yes   \n",
      "products         :   channel          :   No    \n",
      "cash             :   liquidity        :   No    \n",
      "share            :   use              :   No    \n",
      "share            :   earn             :   No    \n",
      "expense          :   line             :   No    \n",
      "provide          :   african          :   No    \n",
      "revenues         :   income           :   No    \n",
      "nine             :   compare          :   No    \n",
      "comparable       :   company          :   No    \n",
      "million          :   unused           :   No    \n"
     ]
    }
   ],
   "source": [
    "# View predictions for data from the training set\n",
    "\n",
    "# Get 30 random Xs, ys\n",
    "testSize = 30\n",
    "testSampleIndex = np.random.randint(0, X_train.shape[0], testSize)\n",
    "testX = X_train[testSampleIndex]\n",
    "testy = y_train[testSampleIndex]\n",
    "\n",
    "# Get predictions\n",
    "scores = model.forward(torch.LongTensor(testX))\n",
    "yPred = np.argmax(scores.detach().numpy(), axis=1)\n",
    "yPredWords = [inverse_vocabulary[i] for i in yPred]\n",
    "\n",
    "# Get actuals\n",
    "yTrueWords = [inverse_vocabulary[i] for i in testy]\n",
    "\n",
    "# Print\n",
    "print(f'{\"Predicted\":14}   :   {\"Actual\":14}   :   {\"Equal?\":6}')\n",
    "print('-------------------------------------------------')\n",
    "for p, a in zip(yPredWords, yTrueWords):\n",
    "    print(f'{p:14}   :   {a:14}   :   {\"Yes\" if p == a else \"No\" :6}')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the word vectors from a model as a .npy file (numpy array)\n",
    "\n",
    "np.save('model_subsample_20M_2_vectors.npy', model.embed.weight.data.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to compute loss on 10000 rows: 5.13755202293396 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "numRows = 10000\n",
    "loss = record_loss(\n",
    "    torch.LongTensor(X_full[:numRows]), \n",
    "    torch.LongTensor(y_full[:numRows]), \n",
    "    model, \n",
    "    F.cross_entropy, \n",
    "    None,\n",
    ")\n",
    "end = time.time()\n",
    "print(f'Time to compute loss on { numRows } rows: { end - start } seconds')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
